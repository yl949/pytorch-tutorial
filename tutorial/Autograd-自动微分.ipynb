{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd: 自动微分\n",
    "在PyTorch的所有神经网络的核心就是<font color=red>autograd</font>包。让我们先简要地看看它，然后我们会训练我们的第一个神经网络。\n",
    "\n",
    "<font color=red>autograd</font>包为张量上的所有操作提供自动微分。它是一个动态定义的框架。这意味着你的反向传播是由你的代码是如何运行而定义的，并且每一次迭代都可以不同。\n",
    "\n",
    "让我们用一些例子来看看更多关于它的简单的术语。\n",
    "\n",
    "## Tensor\n",
    "torch.Tensor是包的核心类。如果将属性 <font color=red>.requires_grad</font> 设置为 <font color=red>True</font>，它就开始追踪跟其相关的所有操作。当你完成了你的计算时，可以调用 <font color=red>.backward()</font> 则所有的梯度计算将被自动完成。这个张量的梯度将被累加到 <font color=red>.grad</font> 属性中。\n",
    "\n",
    "要停止一个tensor的追踪历史，你可以调用 <font color=red>.detach()</font> 方法来把它和计算历史分离开来，并且防止之后的计算不会被追踪。\n",
    "\n",
    "为了防止追踪历史（和内存的使用），你可以用 <font color=red>with torch.no_grad()</font>: 来打包你的代码形成在 <font color=red>with torch.no_grad()</font>: 里面的代码块。这个方法在评估我们模型的时候显得尤为重要。因为模型可能拥有很多可训练的参数，且这些参数的 *requires_grad=True*，但是在评估阶段我们并不需要梯度。\n",
    "\n",
    "这里还有一个类是对于 autograd 实现相当重要的，他就是 - a <font color=red>Function</font>.\n",
    "\n",
    "<font color=red>Tensor</font> 和 <font color=red>Function</font> 相互内联从而建立了一个无环图。无环图编码了一个完整的计算历史。每个 tensor 都有个 <font color=red>.grad_fn</font> 的属性。这个属性能够推测出一个创建一个Tensor的 <font color=red>Function</font>（除非 Tensors 是被用户创建的，那么他们的 grad_fn 是 None）。\n",
    "\n",
    "如果你想要计算微分，你可以在一个 Tensor 上调用 <font color=red>.backward()</font>。如果 Tensor 是个标量 （例如，它只是承载了一个元素数据）， 你不需要给定任何参数给 <font color=red>backward()</font> ，但是当 Tensor 有多个元素的数据时，你需要制定一个和该 tensor shape 对应的 tensor 作为梯度参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建一个 tensor 然后把 require_grad 置为 True 来追踪和它相关的计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  1.],\n",
      "        [ 1.,  1.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 tensor 上面做个操作（计算）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.,  3.],\n",
      "        [ 3.,  3.]])\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>y</font>是有一个操作（加法）产生的结果，所以它拥有一个 <font color=red>grad_fn</font>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x115095198>\n"
     ]
    }
   ],
   "source": [
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在y上面做更多的操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 27.,  27.],\n",
      "        [ 27.,  27.]]) tensor(27.)\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>.requires\\_grad\\_(...)</font> 会原位（in-place）改变 Tensors 的 requires_grad。当我们没有提供 requires_grad 的信息，默认的配置是 <font color=red>True</font>。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x11506c588>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度\n",
    "现在让我们来反向传播。因为 <font color=red>out</font> 只包含一个标量，所以 <font color=red>out.backward()</font> 和 <font color=red>out.backward( torch.tensor(1) )</font> 是等价的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4.5000,  4.5000],\n",
      "        [ 4.5000,  4.5000]])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以得到一个4.5的2x2矩阵。如果我们把 <font color=red>out</font> 这个 Tensor 称为 “o“ 的话。那么$$ o=\\frac{1}{4}\\sum_{i} z_i,z_i=3(x_i + 2)^2$$。所以我们知道o对于x中某个位置求偏导应该为$$\\frac{3}{2}(x_i + 2)$$所以，偏导为9除以2等于4.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你可以用 autograd 做很多疯狂的事！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  762.4423, -1363.4224,   -77.9226])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  102.4000,  1024.0000,     0.1024])\n"
     ]
    }
   ],
   "source": [
    "gradients = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
    "y.backward(gradients)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你可以通过使用 <font color=red>.requires_grad=True by wrapping the code block in with torch.no_grad():</font> 停止 autograd 不在追踪 Tensors 的历史"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 后续阅读：\n",
    "关于 <font color=red>autograd</font> 和 <font color=red>Function</font> 的文档位于https://pytorch.org/docs/stable/autograd.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
