{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch 简介\n",
    "## 介绍 Torch's tensor 库\n",
    "深度学习中我们操作的是 tensor。tensor 是向量和矩阵的一种泛化延伸。向量是 1d 的 tensor，矩阵是 2d 的 tensor。首先让我们来看看 tensor 能做些什么。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1114bbe50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建 tensors\n",
    "我们可以通过 torch.Tensor() 方法把 Python list 变成 Tensors。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.,  2.,  3.])\n",
      "tensor([[ 1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.]])\n",
      "tensor([[[ 1.,  2.],\n",
      "         [ 3.,  4.]],\n",
      "\n",
      "        [[ 5.,  6.],\n",
      "         [ 7.,  8.]]])\n",
      "tensor(1.)\n",
      "1.0\n",
      "tensor([ 1.,  2.,  3.])\n",
      "tensor([[ 1.,  2.],\n",
      "        [ 3.,  4.]])\n"
     ]
    }
   ],
   "source": [
    "# torch.tensor(data) creates a torch.Tensor object with the given data\n",
    "V_data = [1., 2., 3.]\n",
    "V = torch.tensor(V_data)\n",
    "print(V)\n",
    "\n",
    "# Creats a matrix\n",
    "M_data = [[1., 2., 3.], [4., 5., 6.]]\n",
    "M = torch.tensor(M_data)\n",
    "print(M)\n",
    "\n",
    "# Create a 3D tensor of size 2x2x2.\n",
    "T_data = [[[1., 2.], [3., 4.]],\n",
    "          [[5., 6.], [7., 8.]]]\n",
    "T = torch.tensor(T_data)\n",
    "print(T)\n",
    "\n",
    "# Index into V and get a scalar (0 dimensional tensor)\n",
    "print(V[0])\n",
    "# Get a Python number from it\n",
    "print(V[0].item())\n",
    "\n",
    "# Index into M and get a vector\n",
    "print(M[0])\n",
    "\n",
    "# Index into T and get a matrix\n",
    "print(T[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你也可以创建其他数据类型的 tensor。具体请查看官方文档。默认情况下，我们创建的 tensors 都是 Float 型的。如果想要创建整型的 tensors 可以尝试使用 torch.LongTensor()。一般而言，Float 和 Long 是最常用的。\n",
    "\n",
    "你可以创建一个多维随机数据的 tensor 通过使用 torch.randn()。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.size() is torch.Size([3, 4, 5])\n",
      "tensor([[[ 0.7575, -0.4068, -0.1277,  0.2804,  0.0375],\n",
      "         [-0.6378, -0.8148, -0.6895,  0.7705, -1.0739],\n",
      "         [-0.2015, -0.5603,  0.6817, -0.5170,  1.7902],\n",
      "         [ 0.5877,  0.2505, -0.7930,  0.5231,  1.2236]],\n",
      "\n",
      "        [[-0.9458, -1.3529,  3.3837, -2.4044, -0.3891],\n",
      "         [-0.0796,  0.7605, -1.0025,  0.9462,  0.3512],\n",
      "         [ 1.5728,  1.7185, -0.0594, -2.4919,  0.2423],\n",
      "         [ 0.2883, -0.1095,  0.3126,  1.5038,  0.5038]],\n",
      "\n",
      "        [[ 0.6223, -0.4481, -0.2856,  0.3880,  0.2352],\n",
      "         [ 1.9142,  1.8364,  1.3245, -0.0705,  0.3470],\n",
      "         [-0.6537,  1.5586,  0.2186, -0.5743,  1.4571],\n",
      "         [ 1.7710, -2.0173,  0.4235,  0.5730, -1.7962]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn((3, 4, 5))\n",
    "print(\"x.size() is {}\".format(x.size()))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor 之间的操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5.,  7.,  9.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1., 2., 3.])\n",
    "y = torch.tensor([4., 5., 6.])\n",
    "z = x + y\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以查看官方文档看更多的操作。这里的操作不止局限于数学运算，还有很多的扩展。\n",
    "\n",
    "其中，concatenation 是一个很有用的操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 5])\n",
      "torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "# By default, it concatecates along the first axis (concatecate rows)\n",
    "x_1 = torch.randn(2, 5)\n",
    "y_1 = torch.randn(3, 5)\n",
    "z_1 = torch.cat([x_1, y_1])\n",
    "print(z_1.size())\n",
    "\n",
    "# Concatenate columns:\n",
    "x_2 = torch.randn(2, 3)\n",
    "y_2 = torch.randn(2, 5)\n",
    "z_2 = torch.cat([x_2, y_2], 1)\n",
    "print(z_2.size())\n",
    "\n",
    "# If your tensors are not compatible, torch will complain.  Uncomment to see the error\n",
    "# torch.cat([x_1, x_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 改变 tensor 的形状（Reshaping Tensors）\n",
    "用 .view() 来改变 tensor 的形状。这个方法很常用。因为在深度学习中，很多模块要求特定尺寸的输入。所以在输入某个模块之前，你需要改变 tensor 的尺寸来适应输入大小的要求。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.9385,  1.4253,  1.5083,  0.1054],\n",
      "         [-1.6050, -0.1064,  0.2466,  0.6125],\n",
      "         [-0.7129, -0.0639,  1.0757, -0.5536]],\n",
      "\n",
      "        [[-1.6160,  0.0934, -1.3898, -0.3105],\n",
      "         [ 0.2693, -0.4886,  1.3694,  0.4539],\n",
      "         [-0.0498,  0.3745,  1.4389,  1.4151]]])\n",
      "tensor([[ 0.9385,  1.4253,  1.5083,  0.1054, -1.6050, -0.1064,  0.2466,\n",
      "          0.6125, -0.7129, -0.0639,  1.0757, -0.5536],\n",
      "        [-1.6160,  0.0934, -1.3898, -0.3105,  0.2693, -0.4886,  1.3694,\n",
      "          0.4539, -0.0498,  0.3745,  1.4389,  1.4151]])\n",
      "torch.Size([2, 3, 4])\n",
      "tensor([[ 0.9385,  1.4253,  1.5083,  0.1054, -1.6050, -0.1064,  0.2466,\n",
      "          0.6125, -0.7129, -0.0639,  1.0757, -0.5536],\n",
      "        [-1.6160,  0.0934, -1.3898, -0.3105,  0.2693, -0.4886,  1.3694,\n",
      "          0.4539, -0.0498,  0.3745,  1.4389,  1.4151]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 3, 4)\n",
    "print(x)\n",
    "print(x.view(2, 12)) # Reshape to 2 rows, 12 columns\n",
    "# Same as above. If one of the dimensions is -1, its size can be inferred\n",
    "print(x.view(2, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算图和自动微分\n",
    "计算图的概念对于高效的深度学习编程来说尤为重要。因为他可以让你免于繁琐的反向梯度传播的计算编写的痛苦。计算图定义了输出的 tensors 是输入的 tensors 通过哪些运算得到的。即它可以跟踪输出 tensors 的计算历史。这样就为自动计算梯度提供了可能。在程序中，我们通过 requires_grad 这个标志来决定是否跟踪一个 tensor 的计算历史。\n",
    "\n",
    "首先，对于程序员来说，我们只关心 torch.Tensor 中的数据以及它的 shape。当我们把两个 tensors 相加得到另一个 tensor。如果我们得到这个 tensor，我们不知道 tensor 是怎么来的（它有可能是从文件中读取的，有可能几个 tensors 相乘得到的等等）。但是，如果 requires_grad=True。则我们能够追踪这个 output 的 tensor 是如何被创造出来的。\n",
    "\n",
    "接下来看一小段代码加以理解："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5.,  7.,  9.])\n",
      "<AddBackward1 object at 0x114ef4cf8>\n"
     ]
    }
   ],
   "source": [
    "# Tensor factory methods have a ''requires_grad'' flag\n",
    "x = torch.tensor([1., 2., 3.], requires_grad=True)\n",
    "\n",
    "# With requires_grad=True, you can still do all \n",
    "# the operations you previously could\n",
    "y = torch.tensor([4., 5., 6.], requires_grad=True)\n",
    "z = x + y\n",
    "print(z)\n",
    "\n",
    "# BUT z knows something extra\n",
    "print(z.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以，你知道 z 是通过相加操作得到的，而不是从文件或者相乘或者指数操作等得到的结果。只要你顺着 grad_fn 方向继续下去，你就能找到你的 x 和 y。\n",
    "\n",
    "但是这个如何帮助我们计算梯度呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(21.)\n",
      "<SumBackward0 object at 0x114ef4dd8>\n"
     ]
    }
   ],
   "source": [
    "# Lets sum up all the entries in z\n",
    "s = z.sum()\n",
    "print(s)\n",
    "print(s.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过微积分的知识我们可以知道，s 对于 x 的第一个元素的微分为 1（可以通过链式法则求解）。那么接下来我们来验证一下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.,  1.,  1.])\n",
      "tensor([ 1.,  1.,  1.])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad) # before backward check for gradient of x\n",
    "# if x.grad is not None, uncomment to assign x.grad = None\n",
    "# x.grad = None\n",
    "s.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "理解下面的代码对于深度学习的编码很总要："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False False\n",
      "None\n",
      "<AddBackward1 object at 0x114f110b8>\n",
      "True\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 2)\n",
    "y = torch.randn(2, 2)\n",
    "# By default, user created Tensors have ``requires_grad=False``\n",
    "print(x.requires_grad, y.requires_grad)\n",
    "z = x + y\n",
    "# So you can't backprop through z\n",
    "print(z.grad_fn)\n",
    "\n",
    "# ``.requires_grad_( ... )`` changes an existing Tensor's ``requires_grad``\n",
    "# flag in-place. The input flag defaults to ``True`` if not given.\n",
    "x = x.requires_grad_()\n",
    "y = y.requires_grad_()\n",
    "# z contains enough information to compute gradients, as we saw above\n",
    "z = x + y\n",
    "print(z.grad_fn)\n",
    "# If any input to an operation has ``requires_grad=True``, so will the output\n",
    "print(z.requires_grad)\n",
    "\n",
    "# Now z has the computation history that relates itself to x and y\n",
    "# Can we just take its values, and **detach** it from its history?\n",
    "new_z = z.detach()\n",
    "\n",
    "# ... does new_z have information to backprop to x and y?\n",
    "# NO!\n",
    "print(new_z.grad_fn)\n",
    "# And how could it? ``z.detach()`` returns a tensor that shares the same storage\n",
    "# as ``z``, but with the computation history forgotten. It doesn't know anything\n",
    "# about how it was computed.\n",
    "# In essence, we have broken the Tensor away from its past history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你可以通过 .requires_grad=True by wrapping the code block in with torch.no_grad(): 来停止追踪 tensors 的计算历史"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
