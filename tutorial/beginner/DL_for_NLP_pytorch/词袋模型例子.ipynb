{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这一个教程主要以一个词袋的模型来介绍下如何用 pytorch 搭建 Network Components。详细信息可以参考[官方文档](https://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html)。官方文档罗列了一些有关深度学习的基础知识以及词袋模型的简介。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10b07f8d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'me': 0, 'gusta': 1, 'comer': 2, 'en': 3, 'la': 4, 'cafeteria': 5, 'Give': 6, 'it': 7, 'to': 8, 'No': 9, 'creo': 10, 'que': 11, 'sea': 12, 'una': 13, 'buena': 14, 'idea': 15, 'is': 16, 'not': 17, 'a': 18, 'good': 19, 'get': 20, 'lost': 21, 'at': 22, 'Yo': 23, 'si': 24, 'on': 25}\n",
      "Before training: \n",
      "The log_pred is :  tensor([[-0.6311, -0.7593]])\n",
      "The label is:  SPANISH\n",
      "The log_pred is :  tensor([[-1.0110, -0.4523]])\n",
      "The label is:  ENGLISH\n",
      "----------\n",
      "Training...\n",
      "epoch: 0, loss: 1.1032856702804565\n",
      "epoch: 0, loss: 0.8261889219284058\n",
      "epoch: 0, loss: 0.6373709440231323\n",
      "epoch: 0, loss: 1.1923892498016357\n",
      "epoch: 1, loss: 0.7105923891067505\n",
      "epoch: 1, loss: 0.5002872943878174\n",
      "epoch: 1, loss: 0.5397118330001831\n",
      "epoch: 1, loss: 0.37703192234039307\n",
      "epoch: 2, loss: 0.45892250537872314\n",
      "epoch: 2, loss: 0.3762458860874176\n",
      "epoch: 2, loss: 0.3831324279308319\n",
      "epoch: 2, loss: 0.20928728580474854\n",
      "epoch: 3, loss: 0.325185626745224\n",
      "epoch: 3, loss: 0.30379316210746765\n",
      "epoch: 3, loss: 0.2816413938999176\n",
      "epoch: 3, loss: 0.14551131427288055\n",
      "epoch: 4, loss: 0.24921923875808716\n",
      "epoch: 4, loss: 0.2538103461265564\n",
      "epoch: 4, loss: 0.2191002368927002\n",
      "epoch: 4, loss: 0.11134561151266098\n",
      "epoch: 5, loss: 0.201474130153656\n",
      "epoch: 5, loss: 0.2171512395143509\n",
      "epoch: 5, loss: 0.1781632900238037\n",
      "epoch: 5, loss: 0.08997014164924622\n",
      "epoch: 6, loss: 0.1689504235982895\n",
      "epoch: 6, loss: 0.18924131989479065\n",
      "epoch: 6, loss: 0.14967261254787445\n",
      "epoch: 6, loss: 0.07534179091453552\n",
      "epoch: 7, loss: 0.14543373882770538\n",
      "epoch: 7, loss: 0.16737964749336243\n",
      "epoch: 7, loss: 0.1288318932056427\n",
      "epoch: 7, loss: 0.06471665948629379\n",
      "epoch: 8, loss: 0.12765420973300934\n",
      "epoch: 8, loss: 0.14985233545303345\n",
      "epoch: 8, loss: 0.11297746747732162\n",
      "epoch: 8, loss: 0.0566609762609005\n",
      "epoch: 9, loss: 0.1137448400259018\n",
      "epoch: 9, loss: 0.13552293181419373\n",
      "epoch: 9, loss: 0.10053513199090958\n",
      "epoch: 9, loss: 0.050351161509752274\n",
      "epoch: 10, loss: 0.1025669053196907\n",
      "epoch: 10, loss: 0.12361149489879608\n",
      "epoch: 10, loss: 0.09052272140979767\n",
      "epoch: 10, loss: 0.04528035223484039\n",
      "epoch: 11, loss: 0.09338774532079697\n",
      "epoch: 11, loss: 0.11356765776872635\n",
      "epoch: 11, loss: 0.08229856938123703\n",
      "epoch: 11, loss: 0.04111963510513306\n",
      "epoch: 12, loss: 0.08571510761976242\n",
      "epoch: 12, loss: 0.10499309748411179\n",
      "epoch: 12, loss: 0.0754268616437912\n",
      "epoch: 12, loss: 0.037646498531103134\n",
      "epoch: 13, loss: 0.07920606434345245\n",
      "epoch: 13, loss: 0.09759348630905151\n",
      "epoch: 13, loss: 0.06960184127092361\n",
      "epoch: 13, loss: 0.03470510244369507\n",
      "epoch: 14, loss: 0.0736144632101059\n",
      "epoch: 14, loss: 0.09114678204059601\n",
      "epoch: 14, loss: 0.06460292637348175\n",
      "epoch: 14, loss: 0.032183121889829636\n",
      "epoch: 15, loss: 0.06875906139612198\n",
      "epoch: 15, loss: 0.08548299223184586\n",
      "epoch: 15, loss: 0.060267124325037\n",
      "epoch: 15, loss: 0.02999758906662464\n",
      "epoch: 16, loss: 0.06450338661670685\n",
      "epoch: 16, loss: 0.08046968281269073\n",
      "epoch: 16, loss: 0.056471455842256546\n",
      "epoch: 16, loss: 0.0280859787017107\n",
      "epoch: 17, loss: 0.06074279174208641\n",
      "epoch: 17, loss: 0.0760023444890976\n",
      "epoch: 17, loss: 0.05312148854136467\n",
      "epoch: 17, loss: 0.026400232687592506\n",
      "epoch: 18, loss: 0.05739562585949898\n",
      "epoch: 18, loss: 0.07199747860431671\n",
      "epoch: 18, loss: 0.05014345794916153\n",
      "epoch: 18, loss: 0.024902891367673874\n",
      "epoch: 19, loss: 0.054397277534008026\n",
      "epoch: 19, loss: 0.06838765740394592\n",
      "epoch: 19, loss: 0.04747900739312172\n",
      "epoch: 19, loss: 0.023564264178276062\n",
      "epoch: 20, loss: 0.05169595405459404\n",
      "epoch: 20, loss: 0.06511778384447098\n",
      "epoch: 20, loss: 0.04508131369948387\n",
      "epoch: 20, loss: 0.022360576316714287\n",
      "epoch: 21, loss: 0.04924960434436798\n",
      "epoch: 21, loss: 0.06214244291186333\n",
      "epoch: 21, loss: 0.04291238635778427\n",
      "epoch: 21, loss: 0.021272560581564903\n",
      "epoch: 22, loss: 0.04702376946806908\n",
      "epoch: 22, loss: 0.059423964470624924\n",
      "epoch: 22, loss: 0.04094114154577255\n",
      "epoch: 22, loss: 0.020284388214349747\n",
      "epoch: 23, loss: 0.0449899286031723\n",
      "epoch: 23, loss: 0.05693073198199272\n",
      "epoch: 23, loss: 0.03914181515574455\n",
      "epoch: 23, loss: 0.019383050501346588\n",
      "epoch: 24, loss: 0.04312428832054138\n",
      "epoch: 24, loss: 0.054636117070913315\n",
      "epoch: 24, loss: 0.03749298304319382\n",
      "epoch: 24, loss: 0.01855762116611004\n",
      "epoch: 25, loss: 0.04140684753656387\n",
      "epoch: 25, loss: 0.05251747742295265\n",
      "epoch: 25, loss: 0.035976577550172806\n",
      "epoch: 25, loss: 0.017798971384763718\n",
      "epoch: 26, loss: 0.03982061520218849\n",
      "epoch: 26, loss: 0.05055546388030052\n",
      "epoch: 26, loss: 0.03457731381058693\n",
      "epoch: 26, loss: 0.01709936559200287\n",
      "epoch: 27, loss: 0.03835112601518631\n",
      "epoch: 27, loss: 0.048733439296483994\n",
      "epoch: 27, loss: 0.033282164484262466\n",
      "epoch: 27, loss: 0.01645219326019287\n",
      "epoch: 28, loss: 0.03698596730828285\n",
      "epoch: 28, loss: 0.047037068754434586\n",
      "epoch: 28, loss: 0.03208000212907791\n",
      "epoch: 28, loss: 0.015851810574531555\n",
      "epoch: 29, loss: 0.03571440652012825\n",
      "epoch: 29, loss: 0.04545384272933006\n",
      "epoch: 29, loss: 0.030961165204644203\n",
      "epoch: 29, loss: 0.015293355099856853\n",
      "epoch: 30, loss: 0.034527141600847244\n",
      "epoch: 30, loss: 0.043972890824079514\n",
      "epoch: 30, loss: 0.029917318373918533\n",
      "epoch: 30, loss: 0.014772599563002586\n",
      "epoch: 31, loss: 0.033416084945201874\n",
      "epoch: 31, loss: 0.04258467257022858\n",
      "epoch: 31, loss: 0.028941184282302856\n",
      "epoch: 31, loss: 0.014285874553024769\n",
      "epoch: 32, loss: 0.03237413987517357\n",
      "epoch: 32, loss: 0.04128079488873482\n",
      "epoch: 32, loss: 0.02802642248570919\n",
      "epoch: 32, loss: 0.013829962350428104\n",
      "epoch: 33, loss: 0.03139503672719002\n",
      "epoch: 33, loss: 0.04005381464958191\n",
      "epoch: 33, loss: 0.027167415246367455\n",
      "epoch: 33, loss: 0.013402041047811508\n",
      "epoch: 34, loss: 0.030473262071609497\n",
      "epoch: 34, loss: 0.03889719024300575\n",
      "epoch: 34, loss: 0.026359232142567635\n",
      "epoch: 34, loss: 0.012999624013900757\n",
      "epoch: 35, loss: 0.02960393764078617\n",
      "epoch: 35, loss: 0.037805069237947464\n",
      "epoch: 35, loss: 0.025597529485821724\n",
      "epoch: 35, loss: 0.012620498426258564\n",
      "epoch: 36, loss: 0.028782736510038376\n",
      "epoch: 36, loss: 0.036772191524505615\n",
      "epoch: 36, loss: 0.02487839385867119\n",
      "epoch: 36, loss: 0.012262719683349133\n",
      "epoch: 37, loss: 0.02800573781132698\n",
      "epoch: 37, loss: 0.03579391539096832\n",
      "epoch: 37, loss: 0.024198384955525398\n",
      "epoch: 37, loss: 0.011924534104764462\n",
      "epoch: 38, loss: 0.027269499376416206\n",
      "epoch: 38, loss: 0.03486604988574982\n",
      "epoch: 38, loss: 0.02355438657104969\n",
      "epoch: 38, loss: 0.011604386381804943\n",
      "epoch: 39, loss: 0.0265708826482296\n",
      "epoch: 39, loss: 0.033984798938035965\n",
      "epoch: 39, loss: 0.022943628951907158\n",
      "epoch: 39, loss: 0.011300872080028057\n",
      "epoch: 40, loss: 0.02590707689523697\n",
      "epoch: 40, loss: 0.03314676135778427\n",
      "epoch: 40, loss: 0.02236361615359783\n",
      "epoch: 40, loss: 0.011012740433216095\n",
      "epoch: 41, loss: 0.025275567546486855\n",
      "epoch: 41, loss: 0.03234882652759552\n",
      "epoch: 41, loss: 0.021812083199620247\n",
      "epoch: 41, loss: 0.010738840326666832\n",
      "epoch: 42, loss: 0.024674033746123314\n",
      "epoch: 42, loss: 0.03158821910619736\n",
      "epoch: 42, loss: 0.0212869793176651\n",
      "epoch: 42, loss: 0.010478165931999683\n",
      "epoch: 43, loss: 0.024100402370095253\n",
      "epoch: 43, loss: 0.030862364917993546\n",
      "epoch: 43, loss: 0.020786456763744354\n",
      "epoch: 43, loss: 0.010229774750769138\n",
      "epoch: 44, loss: 0.023552779108285904\n",
      "epoch: 44, loss: 0.030168965458869934\n",
      "epoch: 44, loss: 0.020308848470449448\n",
      "epoch: 44, loss: 0.009992825798690319\n",
      "epoch: 45, loss: 0.023029426112771034\n",
      "epoch: 45, loss: 0.029505904763936996\n",
      "epoch: 45, loss: 0.01985260657966137\n",
      "epoch: 45, loss: 0.009766535833477974\n",
      "epoch: 46, loss: 0.022528788074851036\n",
      "epoch: 46, loss: 0.0288712028414011\n",
      "epoch: 46, loss: 0.019416334107518196\n",
      "epoch: 46, loss: 0.00955022219568491\n",
      "epoch: 47, loss: 0.022049391642212868\n",
      "epoch: 47, loss: 0.028263138607144356\n",
      "epoch: 47, loss: 0.018998749554157257\n",
      "epoch: 47, loss: 0.009343234822154045\n",
      "epoch: 48, loss: 0.021589932963252068\n",
      "epoch: 48, loss: 0.027680030092597008\n",
      "epoch: 48, loss: 0.01859869435429573\n",
      "epoch: 48, loss: 0.009144983254373074\n",
      "epoch: 49, loss: 0.021149208769202232\n",
      "epoch: 49, loss: 0.027120398357510567\n",
      "epoch: 49, loss: 0.018215065822005272\n",
      "epoch: 49, loss: 0.008954922668635845\n",
      "epoch: 50, loss: 0.02072606049478054\n",
      "epoch: 50, loss: 0.026582850143313408\n",
      "epoch: 50, loss: 0.01784689910709858\n",
      "epoch: 50, loss: 0.008772567845880985\n",
      "epoch: 51, loss: 0.020319482311606407\n",
      "epoch: 51, loss: 0.02606610767543316\n",
      "epoch: 51, loss: 0.01749325729906559\n",
      "epoch: 51, loss: 0.008597460575401783\n",
      "epoch: 52, loss: 0.01992851309478283\n",
      "epoch: 52, loss: 0.025568995624780655\n",
      "epoch: 52, loss: 0.017153315246105194\n",
      "epoch: 52, loss: 0.008429172448813915\n",
      "epoch: 53, loss: 0.01955227740108967\n",
      "epoch: 53, loss: 0.025090418756008148\n",
      "epoch: 53, loss: 0.016826286911964417\n",
      "epoch: 53, loss: 0.008267311379313469\n",
      "epoch: 54, loss: 0.01918996311724186\n",
      "epoch: 54, loss: 0.02462935447692871\n",
      "epoch: 54, loss: 0.016511455178260803\n",
      "epoch: 54, loss: 0.008111524395644665\n",
      "epoch: 55, loss: 0.01884080097079277\n",
      "epoch: 55, loss: 0.024184878915548325\n",
      "epoch: 55, loss: 0.016208147630095482\n",
      "epoch: 55, loss: 0.007961473427712917\n",
      "epoch: 56, loss: 0.018504086881875992\n",
      "epoch: 56, loss: 0.02375609241425991\n",
      "epoch: 56, loss: 0.015915747731924057\n",
      "epoch: 56, loss: 0.007816856727004051\n",
      "epoch: 57, loss: 0.018179189413785934\n",
      "epoch: 57, loss: 0.023342199623584747\n",
      "epoch: 57, loss: 0.015633678063750267\n",
      "epoch: 57, loss: 0.007677368354052305\n",
      "epoch: 58, loss: 0.017865465953946114\n",
      "epoch: 58, loss: 0.02294243313372135\n",
      "epoch: 58, loss: 0.015361395664513111\n",
      "epoch: 58, loss: 0.0075427573174238205\n",
      "epoch: 59, loss: 0.017562370747327805\n",
      "epoch: 59, loss: 0.022556083276867867\n",
      "epoch: 59, loss: 0.015098413452506065\n",
      "epoch: 59, loss: 0.0074127609841525555\n",
      "epoch: 60, loss: 0.017269369214773178\n",
      "epoch: 60, loss: 0.02218247763812542\n",
      "epoch: 60, loss: 0.014844254590570927\n",
      "epoch: 60, loss: 0.007287150714546442\n",
      "epoch: 61, loss: 0.016985969617962837\n",
      "epoch: 61, loss: 0.02182101458311081\n",
      "epoch: 61, loss: 0.0145984822884202\n",
      "epoch: 61, loss: 0.007165716961026192\n",
      "epoch: 62, loss: 0.016711700707674026\n",
      "epoch: 62, loss: 0.021471109241247177\n",
      "epoch: 62, loss: 0.014360691420733929\n",
      "epoch: 62, loss: 0.007048242259770632\n",
      "epoch: 63, loss: 0.016446124762296677\n",
      "epoch: 63, loss: 0.021132206544280052\n",
      "epoch: 63, loss: 0.014130501076579094\n",
      "epoch: 63, loss: 0.006934541277587414\n",
      "epoch: 64, loss: 0.016188856214284897\n",
      "epoch: 64, loss: 0.020803797990083694\n",
      "epoch: 64, loss: 0.013907556422054768\n",
      "epoch: 64, loss: 0.006824439391493797\n",
      "epoch: 65, loss: 0.015939483419060707\n",
      "epoch: 65, loss: 0.020485416054725647\n",
      "epoch: 65, loss: 0.013691510073840618\n",
      "epoch: 65, loss: 0.006717764772474766\n",
      "epoch: 66, loss: 0.015697671100497246\n",
      "epoch: 66, loss: 0.020176604390144348\n",
      "epoch: 66, loss: 0.013482064940035343\n",
      "epoch: 66, loss: 0.00661435816437006\n",
      "epoch: 67, loss: 0.015463078394532204\n",
      "epoch: 67, loss: 0.019876940175890923\n",
      "epoch: 67, loss: 0.013278904370963573\n",
      "epoch: 67, loss: 0.006514077074825764\n",
      "epoch: 68, loss: 0.015235373750329018\n",
      "epoch: 68, loss: 0.01958601363003254\n",
      "epoch: 68, loss: 0.013081764802336693\n",
      "epoch: 68, loss: 0.006416781339794397\n",
      "epoch: 69, loss: 0.01501427311450243\n",
      "epoch: 69, loss: 0.019303467124700546\n",
      "epoch: 69, loss: 0.012890373356640339\n",
      "epoch: 69, loss: 0.006322340574115515\n",
      "epoch: 70, loss: 0.014799481257796288\n",
      "epoch: 70, loss: 0.0190289206802845\n",
      "epoch: 70, loss: 0.012704478576779366\n",
      "epoch: 70, loss: 0.00623062951490283\n",
      "epoch: 71, loss: 0.01459074392914772\n",
      "epoch: 71, loss: 0.018762072548270226\n",
      "epoch: 71, loss: 0.012523870915174484\n",
      "epoch: 71, loss: 0.00614152941852808\n",
      "epoch: 72, loss: 0.014387790113687515\n",
      "epoch: 72, loss: 0.018502570688724518\n",
      "epoch: 72, loss: 0.01234830729663372\n",
      "epoch: 72, loss: 0.006054931785911322\n",
      "epoch: 73, loss: 0.014190413989126682\n",
      "epoch: 73, loss: 0.018250126391649246\n",
      "epoch: 73, loss: 0.012177584692835808\n",
      "epoch: 73, loss: 0.0059707374311983585\n",
      "epoch: 74, loss: 0.013998364098370075\n",
      "epoch: 74, loss: 0.018004467710852623\n",
      "epoch: 74, loss: 0.012011508457362652\n",
      "epoch: 74, loss: 0.00588884437456727\n",
      "epoch: 75, loss: 0.013811427168548107\n",
      "epoch: 75, loss: 0.01776532083749771\n",
      "epoch: 75, loss: 0.011849884875118732\n",
      "epoch: 75, loss: 0.005809159483760595\n",
      "epoch: 76, loss: 0.013629422523081303\n",
      "epoch: 76, loss: 0.01753241941332817\n",
      "epoch: 76, loss: 0.011692541651427746\n",
      "epoch: 76, loss: 0.005731593351811171\n",
      "epoch: 77, loss: 0.013452130369842052\n",
      "epoch: 77, loss: 0.017305541783571243\n",
      "epoch: 77, loss: 0.011539321392774582\n",
      "epoch: 77, loss: 0.005656065884977579\n",
      "epoch: 78, loss: 0.013279394246637821\n",
      "epoch: 78, loss: 0.01708444021642208\n",
      "epoch: 78, loss: 0.011390051804482937\n",
      "epoch: 78, loss: 0.005582496989518404\n",
      "epoch: 79, loss: 0.013111017644405365\n",
      "epoch: 79, loss: 0.01686890795826912\n",
      "epoch: 79, loss: 0.011244579218327999\n",
      "epoch: 79, loss: 0.005510810296982527\n",
      "epoch: 80, loss: 0.012946858070790768\n",
      "epoch: 80, loss: 0.016658727079629898\n",
      "epoch: 80, loss: 0.011102769523859024\n",
      "epoch: 80, loss: 0.005440941080451012\n",
      "epoch: 81, loss: 0.012786759994924068\n",
      "epoch: 81, loss: 0.016453703865408897\n",
      "epoch: 81, loss: 0.010964486747980118\n",
      "epoch: 81, loss: 0.005372809711843729\n",
      "epoch: 82, loss: 0.012630566954612732\n",
      "epoch: 82, loss: 0.01625366508960724\n",
      "epoch: 82, loss: 0.010829590260982513\n",
      "epoch: 82, loss: 0.00530635891482234\n",
      "epoch: 83, loss: 0.012478131800889969\n",
      "epoch: 83, loss: 0.016058417037129402\n",
      "epoch: 83, loss: 0.010697968304157257\n",
      "epoch: 83, loss: 0.005241527687758207\n",
      "epoch: 84, loss: 0.012329326942563057\n",
      "epoch: 84, loss: 0.015867792069911957\n",
      "epoch: 84, loss: 0.010569507256150246\n",
      "epoch: 84, loss: 0.0051782578229904175\n",
      "epoch: 85, loss: 0.012184027582406998\n",
      "epoch: 85, loss: 0.015681631863117218\n",
      "epoch: 85, loss: 0.010444080457091331\n",
      "epoch: 85, loss: 0.005116493906825781\n",
      "epoch: 86, loss: 0.012042097747325897\n",
      "epoch: 86, loss: 0.015499777160584927\n",
      "epoch: 86, loss: 0.01032159011811018\n",
      "epoch: 86, loss: 0.005056179128587246\n",
      "epoch: 87, loss: 0.011903446167707443\n",
      "epoch: 87, loss: 0.015322084538638592\n",
      "epoch: 87, loss: 0.010201936587691307\n",
      "epoch: 87, loss: 0.004997265059500933\n",
      "epoch: 88, loss: 0.011767938733100891\n",
      "epoch: 88, loss: 0.015148412436246872\n",
      "epoch: 88, loss: 0.010085017420351505\n",
      "epoch: 88, loss: 0.0049397083930671215\n",
      "epoch: 89, loss: 0.011635476723313332\n",
      "epoch: 89, loss: 0.014978629536926746\n",
      "epoch: 89, loss: 0.009970742277801037\n",
      "epoch: 89, loss: 0.0048834579065442085\n",
      "epoch: 90, loss: 0.011505958624184132\n",
      "epoch: 90, loss: 0.014812600798904896\n",
      "epoch: 90, loss: 0.009859021753072739\n",
      "epoch: 90, loss: 0.0048284693621098995\n",
      "epoch: 91, loss: 0.011379293166100979\n",
      "epoch: 91, loss: 0.014650200493633747\n",
      "epoch: 91, loss: 0.009749770164489746\n",
      "epoch: 91, loss: 0.004774702712893486\n",
      "epoch: 92, loss: 0.011255375109612942\n",
      "epoch: 92, loss: 0.014491312205791473\n",
      "epoch: 92, loss: 0.009642912074923515\n",
      "epoch: 92, loss: 0.004722112789750099\n",
      "epoch: 93, loss: 0.011134134605526924\n",
      "epoch: 93, loss: 0.014335829764604568\n",
      "epoch: 93, loss: 0.009538360871374607\n",
      "epoch: 93, loss: 0.004670669324696064\n",
      "epoch: 94, loss: 0.011015467345714569\n",
      "epoch: 94, loss: 0.014183643274009228\n",
      "epoch: 94, loss: 0.009436052292585373\n",
      "epoch: 94, loss: 0.0046203299425542355\n",
      "epoch: 95, loss: 0.010899297893047333\n",
      "epoch: 95, loss: 0.014034648425877094\n",
      "epoch: 95, loss: 0.009335912764072418\n",
      "epoch: 95, loss: 0.004571062978357077\n",
      "epoch: 96, loss: 0.010785555467009544\n",
      "epoch: 96, loss: 0.013888747431337833\n",
      "epoch: 96, loss: 0.009237869642674923\n",
      "epoch: 96, loss: 0.00452283164486289\n",
      "epoch: 97, loss: 0.010674157179892063\n",
      "epoch: 97, loss: 0.013745836913585663\n",
      "epoch: 97, loss: 0.009141858667135239\n",
      "epoch: 97, loss: 0.004475607071071863\n",
      "epoch: 98, loss: 0.010565025731921196\n",
      "epoch: 98, loss: 0.013605836778879166\n",
      "epoch: 98, loss: 0.00904782023280859\n",
      "epoch: 98, loss: 0.004429352469742298\n",
      "epoch: 99, loss: 0.01045810617506504\n",
      "epoch: 99, loss: 0.013468654826283455\n",
      "epoch: 99, loss: 0.008955692872405052\n",
      "epoch: 99, loss: 0.004384044092148542\n",
      "----------\n",
      "After training: \n",
      "The log_pred is :  tensor([[-0.1164, -2.2080]])\n",
      "The label is:  SPANISH\n",
      "The log_pred is :  tensor([[-3.2412, -0.0399]])\n",
      "The label is:  ENGLISH\n"
     ]
    }
   ],
   "source": [
    "data = [(\"me gusta comer en la cafeteria\".split(), \"SPANISH\"),\n",
    "        (\"Give it to me\".split(), \"ENGLISH\"),\n",
    "        (\"No creo que sea una buena idea\".split(), \"SPANISH\"),\n",
    "        (\"No it is not a good idea to get lost at sea\".split(), \"ENGLISH\")]\n",
    "\n",
    "test_data = [(\"Yo creo que si\".split(), \"SPANISH\"),\n",
    "             (\"it is lost on me\".split(), \"ENGLISH\")]\n",
    "\n",
    "# word_to_ix maps each word in the vocab to a unique integer, which will be its\n",
    "# index into the Bag of words vector\n",
    "word_to_ix = {}\n",
    "for sent, _ in data + test_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "print(word_to_ix)\n",
    "\n",
    "VOCAB_SIZE = len(word_to_ix)\n",
    "NUM_LABELS = 2\n",
    "\n",
    "label_to_ix = {'SPANISH': 0, 'ENGLISH': 1}\n",
    "ix_to_label = {0: 'SPANISH', 1: 'ENGLISH'}\n",
    "\n",
    "class BowClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, num_labels):\n",
    "        super(BowClassifier, self).__init__()\n",
    "        self.linear = nn.Linear(vocab_size, num_labels)\n",
    "    \n",
    "    def forward(self, input_word_vec):\n",
    "        output = self.linear(input_word_vec)\n",
    "        pred = F.log_softmax(output, dim=1)\n",
    "        return pred\n",
    "\n",
    "def make_bow_vector(sentence, word_to_ix):\n",
    "    word_vec = torch.zeros(len(word_to_ix))\n",
    "    for word in sentence:\n",
    "        word_vec[word_to_ix[word]] += 1\n",
    "    return word_vec.view(1, -1)\n",
    "\n",
    "def make_label_vector(label, label_to_ix):\n",
    "    label_vec = torch.LongTensor([label_to_ix[label]])\n",
    "    return label_vec\n",
    "\n",
    "model = BowClassifier(VOCAB_SIZE, NUM_LABELS)\n",
    "\n",
    "# we can uncomment to see the size of model prarameters\n",
    "# the first is A, the second is bias\n",
    "# for param in model.parameters():\n",
    "#     print(param.size())\n",
    "\n",
    "# we can feed a sample data to model to see its output.\n",
    "# uncomment to see the result\n",
    "# with torch.no_grad():\n",
    "#     sample = data[0]\n",
    "#     bow_vector = make_bow_vector(sample[0], word_to_ix)\n",
    "#     log_probs = model(bow_vector)\n",
    "#     print(log_probs)\n",
    "\n",
    "# Run on test data before we train, just to see a before-and-after\n",
    "print(\"Before training: \")\n",
    "with torch.no_grad():\n",
    "    for instance, label in test_data:\n",
    "        bow_vec = make_bow_vector(instance, word_to_ix)\n",
    "        log_pred = model(bow_vec)\n",
    "        print(\"The log_pred is : \", log_pred)\n",
    "        ix = torch.max(log_pred, dim=1)[1]\n",
    "        print(\"The label is: \", ix_to_label[ix.item()])\n",
    "\n",
    "print(\"-\" * 10)\n",
    "print(\"Training...\")\n",
    "\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(100):\n",
    "    for sentence, label in data:\n",
    "        bow_vec = make_bow_vector(sentence, word_to_ix)\n",
    "        label_vec = make_label_vector(label, label_to_ix)\n",
    "        log_pred = model(bow_vec)\n",
    "        loss = loss_function(log_pred, label_vec)\n",
    "        print(\"epoch: {}, loss: {}\".format(epoch, loss))\n",
    "        \n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "print(\"-\" * 10)\n",
    "print(\"After training: \")\n",
    "with torch.no_grad():\n",
    "    for instance, label in test_data:\n",
    "        bow_vec = make_bow_vector(instance, word_to_ix)\n",
    "        log_pred = model(bow_vec)\n",
    "        print(\"The log_pred is : \", log_pred)\n",
    "        ix = torch.max(log_pred, dim=1)[1]\n",
    "        print(\"The label is: \", ix_to_label[ix.item()])\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
